---
title: Enterprise PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for <%= vars.product_full %> v1.5.0.

## <a id="v1.5.0"></a>v1.5.0-beta1

**Release Date**: May 16, 2019

### <a id="v1.5.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.5.0 Beta</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>May 16, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.3+, v2.5.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v250.25</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.14.0</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.26.0</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.3.1, v2.4.0.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.0</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
    <tr>
        <td>Compatible BBR version</td>
        <td>v1.5.0+</td>
    </tr>
</table>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a user interface (UI) based on the NSX Policy API. 
PKS v1.5.0 does not support the NSX Policy API. Any objects created via the new Policy-based UI cannot be used with PKS v1.5.0.
If you are installing PKS v1.5.0 with NSX-T v2.4.x, or upgrading to PKS v1.5.0 and NSX-T 2.4.x, 
you must use the "Advanced Networking" tab in NSX Manager to create, read, update, and delete 
all networking objects required for PKS.</p>

<p class="note"><strong>Note:</strong> Ops Manager v2.5.0 and later in the
v2.5 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.4 on Azure.
Before deploying PKS v1.4 on Azure, you must install Ops Manager v2.4.2
or v2.4.3.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

For <%= vars.product_short %> installations on vSphere or on vSphere with NSX-T Data Center, refer to the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a>.</p>

### <a id="v1.5.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Windows Worker-based Cluster</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;<sup>&#42;</sup><sup>&#42;</sup></td>
    <td></td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.  
<sup>&#42;</sup><sup>&#42;</sup> Windows worker-based Kubernetes cluster support in PKS v1.5 requires vSphere with Flannel.

### <a id="v1.5.0-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.5.0 are from PKS v1.3.2 and later.

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

### <a id="v1.5.0-whats-new"></a>What's New

<%= vars.product_short %> v1.5.0 adds the following:

* Operators can provision a Windows worker-based Kubernetes cluster on vSphere with Flannel. 
Windows worker-based clusters in <%= vars.product_short %> 1.5 currently do not support NSX-T integration.


### <a id="v1.5.0-known-issues"></a>Breaking Changes and Known Issues

<%= vars.product_short %> v1.5.0 has the following known issues:

#### <a name="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, in <%= vars.product_short %> v1.4, the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

#### <a id='first-az'></a>Cluster Creation Fails When First AZ Runs out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if your three AZs each have enough resources for ten VMs, and you
create two clusters with four worker VMs each, BOSH creates VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

#### <a id='azure-worker-comm'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after an upgrade to <%= vars.product_short %> v1.4.0.

**Explanation**

<%= vars.product_short %> 1.4.0 uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.
