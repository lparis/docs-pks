---
title: Upgrading Enterprise PKS with NSX-T
owner: PKS
topictype: vspherewithnsxtupgrade
---

<strong><%= modified_date %></strong>

This topic explains how to upgrade <%= vars.product_full %> for environments using vSphere with NSX-T.

## <a id="before"></a>Before You Upgrade

This section describes the activities you must perform before upgrading <%= vars.product_short %>.

### <a id="compatiblity-chart"></a>Consult Compatibility Charts

For information about PKS with NSX-T and Ops Manager compatibility, see the [<%= vars.product_short %> Release Notes](release-notes.html).

### <a id="upgrade-path"></a> Determine Your Upgrade Path

For information about the supported upgrade path, see _Updgrade Path_ section of the [<%= vars.product_short %> Release Notes](release-notes.html#v1.4.0-upgrade).

### <a id="prepare"></a>Prepare to Upgrade

If you have not already, complete all of the steps in the [Upgrade Preparation Checklist for PKS v1.4](checklist.html).

## <a id="upgrade"></a> During the Upgrade

This section describes the steps required to upgrade to <%= vars.product_short %> v1.4 and NSX-T v2.4.

### <a id="upgrade-opsman"></a> Step 1: Upgrade to Ops Manager v2.4.3+

Before you upgrade to PKS v1.4.0, you must upgrade to Ops Manager v2.4.3 or later. For more information, see the [<%= vars.product_short %> Release Notes](release-notes.html).

To upgrade Ops Manager, follow the procedures detailed in [Upgrade Ops Manager and Installed Products to v2.4](https://docs.pivotal.io/pivotalcf/2-4/customizing/upgrading-pcf.html#upgrade_ops).

### <a id="upgrade-pks-to-140"></a> Step 2: Upgrade to PKS v1.4.0

Upgrade the PKS tile from a supported version to to PKS v1.4.0. When you upgrade the PKS tile, the target version of NCP is installed (v2.4.0 in this case). This **must** be done before you upgrade to NSX-T v2.4.x. 
 
To upgrade the PKS tile to v1.4.0, complete the following steps.

1. Download the PKS version from [Pivotal Network](https://network.pivotal.io/products/pivotal-container-service/).

1. Navigate to the Ops Manager Installation Dashboard and click **Import a Product**.

1. Browse to the <%= vars.product_short %> product file and select it. Uploading the file takes several minutes.
  <img src="images/upgrade-nsxt-2.png" alt="Upload PKS product file to Ops Manager">

1. Under the **Import a Product** button, click **+** next to **<%= vars.product_tile %>**. This adds the tile to your staging area.<br><br>
  <img src="images/upgrade-nsxt-3.png" alt="Import the PKS product file">

1. Import the required [Xenial stemcell](https://docs.pivotal.io/pivotalcf/stemcells/#xenial) for PKS v1.4.0 (250.25 or later).
  * On the <%= vars.product_tile %> tile, click on the **Missing stemcell** link.
  * In the **Stemcell Library**, locate **<%= vars.product_tile %>** and note the required stemcell version.
  * Visit the [Stemcells for PCF (Ubuntu Xenial)](https://network.pivotal.io/products/stemcells-ubuntu-xenial/) page on Pivotal Network,
and download the required stemcell version for vSphere.
 * Return to the **Installation Dashboard** in Ops Manager, and click on **Stemcell Library**.
 * On the **Stemcell Library** page, click **Import Stemcell** and select the stemcell file you downloaded from Pivotal Network.
 * Select the <%= vars.product_tile %> product and click **Apply Stemcell to Products**.
 * Verify that Ops Manager successfully applied the stemcell.

1. Return to the **Installation Dashboard** in Ops Manager.

1. Click **Review Pending Changes**.
   For more information about this Ops Manager page, see
    [Reviewing Pending Product Changes](https://docs.pivotal.io/pivotalcf/2-3/customizing/review-pending-changes.html).

1. In the <%= vars.product_tile %> tile, click **Errands**.

1. Under **Post-Deploy Errands**, verify that the listed errands are configured as follows:
  * **NSX-T Validation errand**: Set to **On**
  * **Upgrade all clusters errand**: Set to **Default (On)**
  * **Create pre-defined Wavefront alerts errand**: Set to **Default (Off)**
  * **Run smoke tests**: Set to **On**. The errand uses the PKS Command Line Interface (PKS CLI) to create a Kubernetes cluster and then delete it. If the creation or deletion fails, the errand fails and the installation of the <%= vars.product_tile %> tile is aborted.

    <p class="note warning"><strong>WARNING</strong>: Because of the upgrade requirements for PSK 1.4 with NSX-T 2.4, you will deploy the PKS tile twice. It is up to you if you want to enable the "Upgrade all clusters errand" at this time, or when you perform the second deployment of PKS. If you are performing the upgrade during a maintenance window, it is not necessary to upgrade the Kubernetes clusters at this time, so you can deselect the upgrade all clusters errand for PKS. However, if you want your Kubernetes clusters to be upgraded immediately, ensure that the upgrade all clusters errand is enabled.</p>

1. Click **Apply Changes** to deploy the PKS 1.4.0 tile.

### <a id="upgrade-nsxt"></a> Step 3: Upgrade from NSX-T v2.3.1 to NSX-T v2.4

Upgrade NSX-T from v2.3.1 to v2.4.0.1. See the **Upgrade Path** section of the [Release Notes](./release-notes.html) for information on obtaining the NSX-T 2.4.0.1 hot-patch.

To perform the upgrade, refer to [Upgrading NSX-T Data Center](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-E04242D7-EF09-4601-8906-3FA77FBB06BD.html) in the VMware documentation.

<p class="note"><strong>Note:</strong> Before upgrading NSX-T, make sure you are on the supported vSphere and patch version. Refer to the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&175=&1=">VMware Product Interoperability Matrices</a> for NSX-T v2.4 and vSphere v6.5 and v6.7.</p>

<p class="note"><strong>Note:</strong> When upgrading NSX-T, at the stage that the ESXi Transport Nodes are upgraded ("Hosts"), you may want to create a different host group for each ESXi host in the correct order so that hosts in maintenance mode only get upgraded. In vCenter, put each EXSi Transport Node (TN) host into maintenance mode, 1 at a time. Create the host group for that ESXi host and upgrade only it, then remove it from maintenance mode. Repeat this process for all ESXi TN hosts.</p>

<p class="note"><strong>Note:</strong> Once you upgrade to NSX-T 2.4, the T0 router(s) and all other management plane objects can be seen only from the <em>Advanced Networking Configuration</em> tab. They will not be migrated to the new Policy UI.</p>

<p class="note"><strong>Note:</strong> There are architectural changes in NSX 2.4. The NSX Controller is now a component of the NSX Manager. Once the NSX-T upgrade is complete, you will have a single NSX-T Manager node. Power off the NSX Controllers. At the end of the upgrade, you can delete the NSX Controller VMs. For more information, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-D946F58F-BFB2-4F8E-A979-4C07393299E8.html">Delete NSX Controllers</a> in the NSX-T documentation.</p> 

<p class="note"><strong>Note:</strong> Once the upgrade to NSX 2.4 is complete, you may want to verify that your PKS environment is functioning properly by logging in to PKS and creating a small test cluster. If you cannot do this, troubleshoot the upgrade before proceeding. For more information, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-260D52CB-1B88-4138-A853-0293A7A3B077.html#GUID-260D52CB-1B88-4138-A853-0293A7A3B077">Troubleshooting Upgrade Failures</a> in the NSX-T documentation.</p>

### <a id="add-managers"></a> Step 4: Deploy Two Additional NSX Managers

With NSX-T v2.4, the NSX Controller component is now part of the NSX Manager. Previously the NSX Manager was a singleton, and HA was achieved using multiple NSX Controllers. With NSX-T v2.4, since the standalone NSX Controller component is no longer used, to achieve HA you need to deploy multiple (three) NSX Managers. Refer to the [Upgrading NSX-T Data Center](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-3B986F37-94FE-4CAC-B4AD-9B55D8FE1EC2.html) documentation for guidance on adding additional NSX Managers.

<p class="note"><strong>Note:</strong> When you add additional NSX Managers, the system prompts you to enter a Compute Manager, which is a vCenter Server. For more information, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.3/com.vmware.nsxt.install.doc/GUID-D225CAFC-04D4-44A7-9A09-7C365AAFCA0E.html">Add a Compute Manager</a> in the NSX-T documentation.</p> 

### <a id="add-vip"></a> Step 5: Configure the NSX Manager VIP

Since you have deployed two additional NSX Managers (for a total of three), you need create a virtual IP address that can be used as a single endpoint to access the NSX Management cluster.

To create a VIP for the NSX Management cluster:

- Log in to the NSX Manager interface.
- Go to **System** > **Overview**.
- Select **Virtual IP** > **Edit**.
- Enter a publicly routable IP address, such as `10.40.206.5`.
- Click **Save**.

At this point in time, you can connect to any NSX-T manager using its own IP address, or use the VIP to connect to NSX-T Manager. Both methods work. However, note that the VIP is associated with a single NSX Manager. To determine which NSX Manager the VIP is associated with, select the Virtual IP.

  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-05.png" alt="VIP Association">

### <a id="nsx-cert"></a> Step 6: Generate, Import, and Register a New NSX Manager CA Cert with the Cluster API

Both the BOSH Director tile and the PKS tile expect the NSX Manager CA certificate. However, the current NSX Manager CA certificate is associated with the original NSX Manager IP address. You need to generate a new NSX Manager CA cert using the VIP address, then register this certificate with NSX-T using the Cluster Certificate API.

####<a id='generate-certificate'></a> Generate a New NSX Manager CA Certificate and Private Key

To generate a new NSX Manager CA certificate and private key using the VIP address, follow the instructions below. Make sure you use the VIP address, such as `10.40.206.5`.

Below is an example Certificate Signing Request (CSR) named `nsx-cert.cnf`. In this example, the IP address `10.40.206.5` is the IP address of the VIP. Substitute this IP address with the VIP you generated.

```
[ req ]

default_bits = 2048

distinguished_name = req_distinguished_name

req_extensions = req_ext

prompt = no

[ req_distinguished_name ]

countryName = US

stateOrProvinceName = California

localityName = CA

organizationName = NSX

commonName = 10.40.206.5

[ req_ext ]

subjectAltName = @alt_names


[alt_names]

DNS.1 = 10.40.206.5
```

To generate the certficate and private key using the above CSR, run the following commands:

```
# export NSX_MANAGER_IP_ADDRESS=10.40.206.5

# export NSX_MANAGER_COMMONNAME=10.40.206.5

# openssl req -newkey rsa:2048 -x509 -nodes \

> -keyout nsx.key -new -out nsx.crt -subj /CN=$NSX_MANAGER_COMMONNAME \

> -reqexts SAN -extensions SAN -config <(cat ./nsx-cert.cnf \

>  <(printf "[SAN]\nsubjectAltName=DNS:$NSX_MANAGER_COMMONNAME,IP:$NSX_MANAGER_IP_ADDRESS")) -sha256 -days 365
```

The result is `nsx.crt` and `nsx.key`. You can verify the certificate using the command `# openssl x509 -in nsx.crt -text -noout`.

####<a id='import-certificate'></a> Import the New Certificate to NSX Manager

Complete the following steps to import the certificate to the NSX Manager:

1. Log in to the NSX Manager UI.

1. Navigate to **System > Trust > Certificates**.

1. Click **Import > Import Certificate**.

	<img src="images/nsxt/import-cert.png" alt="Import the NSX Manager CA certificate to the NSX Manager" width="475">
    <p class="note"><strong>Note</strong>: Make sure you select **Import Certificate** and not **Import CA Certificate**</code>.</p>

1. Give the certificate a unique name, such as `NSX-API-CERT-NEW`.
  <p class="note"><strong>Note</strong>: Use a unique name for the new certificate you import. The default NSX Manager CA certificate is typically named <code>NSX-API-CERT</code>.</p>

1. Browse to and select the CA certificate and private key you generated in the previous section of steps.

1. Click **Save**.

 	<img src="images/nsxt/import-cert-2.png" alt="Import the NSX Manager CA certificate to the NSX Manager" width="475"> 

####<a id='register-certificate'></a> Register the New Certificate with NSX Manager

Once you have imported the NSX Manager certificate, register this certificate with the NSX Management cluster using a cURL command against the Cluster Certificate API.

First, create environment variables for the VIP address and the certificate ID. In this example, `10.40.206.5` is the VIP address. The certificate ID is obtained from the NSX Manager UI where you imported the certificate.

```
export NSX_MANAGER_IP_ADDRESS=10.40.206.5

export CERTIFICATE_ID="63bb6646-052c-49df-b603-64d7e5bdb5bf"
```

Next, register the new NSX-T Manager CA cert using a cURL request to the Cluster Certificate API. Substitute `PASSWORD` with the password for NSX Manager.

```
curl --insecure -u admin:'PASSWORD' -X POST "https://$NSX_MANAGER_IP_ADDRESS/api/v1/cluster/api-certificate?action=set_cluster_certificate&certificate_id=$CERTIFICATE_ID"
```

The certificate will be registered with the NSX Manager that the VIP address is associated with. 

To verify, using a browser go to the VIP address of the NSX Manager. Login and check that the new cert is used by the site (accessed using the VIP address). 

To further verify, SSH to each NSX Manager host and run the following two commands. All certificates returned should be the same.

```
get certificate api
get certificate cluster
```

### <a id="upgrade-tiles"></a> Step 7: Update the PKS and BOSH Tiles with the New NSX Manager Cluster Certficate and VIP Address

The last procedure in the upgrade process is to modify the BOSH Tile and the PKS Tile with the new VIP address for the NSX Manager and the new NSX-T Manager CA cert (using VIP info). Apply the changes and ensure that the **Upgrade all clusters errand** is selected, then deploy PKS.

To update the BOSH tile:

1. Log into Ops Manager. 
1. In the BOSH Director tile, select the **vCenter Configuration** tab.
1. In the **NSX Address** field, enter the VIP address for the NSX Management Cluster.
1. In the **NSX CA Cert** field, enter the new CA certificate for the NSX Management Cluster that uses the VIP address.
1. Save the BOSH tile changes.
  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-08.png" alt="Update BOSH with VIP and Cert"> 

To update the PKS tile:

1. Log into Ops Manager. 
1. In the PKS tile, select the **Networking** tab.
1. In the **NSX Manager hostname** field, enter the VIP address for the NSX Management Cluster.
1. In the **NSX Manager CA Cert** field, enter the new CA certificate for the NSX Management Cluster (that uses the VIP address).
1. Save the PKS tile changes.
  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-09.png" alt="Update PKS with VIP and Cert"> 

### <a id="upgrade-k8s"></a> Step 8: Upgrade all Kubernetes Clusters

Once you have updated the PKS and BOSH tiles, apply the changes. Be sure to run the "Upgrade all [Kubernetes] clusters errand". Doing so will allow NCP configurations on all Kubernetes clusters to be updated with the new NSX-T Management Cluster VIP and CA certificate.

To complete the upgrade:

1. Go to the **Installation Dashboard** in Ops Manager.
1. Click **Review Pending Changes.**
1. Expand the **Errands** list for PKS.
1. Ensure that the **Upgrade all clusters errand** is selected.
1. Click **Apply Changes**.
  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-10.png" alt="Upgrade all Kubernetes clusters"> 

### <a id="verify-ncp"></a> Step 9: Verify the Upgrade

Once the upgrade is complete, verify that NCP configuration is automatically updated with the new VIP (instead of individual NSX-T Manager node IP address).

To do this, run a command similar to the following for each Kubernetes cluster (service-instance_UUID):

```
bosh ssh master/0 -d service-instance_d9b662d0-23e1-4239-b641-ed20ee62e692
```

Note the "nsx_api_managers" address. It should be the VIP.

## <a id="post-upgrade"></a> After the Upgrade

After you complete the upgrade to <%= vars.product_short %> v1.4.x and NSX-T v2.3, complete the following verifications and upgrades.

### <a id="update-clis"></a> Update PKS and Kubernetes CLIs

Update the PKS and Kubernetes CLIs on any local machine
where you run commands that interact with your upgraded version of <%= vars.product_short %>.

To update your CLIs, download and re-install the PKS and Kubernetes CLI distributions
that are provided with <%= vars.product_short %> on Pivotal Network.

For more information about installing the CLIs, see the following topics:

* [Installing the PKS CLI](installing-pks-cli.html)

* [Installing the Kubernetes CLI](installing-kubectl-cli.html)

### <a id="verify"></a> Verify Deployment Health

After you apply changes to the <%= vars.product_tile %> tile and the upgrade is complete,
verify that your Kubernetes environment is healthy and confirm that NCP is
running on the master node VM.

To verify the health of your Kubernetes environment and NCP, see [Verifying
Deployment Health](verify-health.html).

